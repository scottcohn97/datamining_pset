---
title: "Exercise 02"
author: "Scott Cohn"
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(scipen = 999) 

library(tidyverse)
library(tidymodels)
library(tidyquant)
library(kknn)
library(lubridate)
library(scales)
library(patchwork)
library(hrbrthemes)
library(kableExtra)
library(gcookbook)
library(mosaic)
library(mosaicData)
```


```{r}
# funcs

read_data <- function(df) {
  #' read data from git url
  #' INPUT: data set name
  #' OUTPUT: dataframe
  full_path <- paste("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/", 
                     df, sep = "")
  df <- read_csv(full_path)
  return(df)
}

```

## Visualization

```{r capmetro_import, eval=FALSE}
# he hasn't uploaded data yet
capmetro <- read_data("capmetro_UT.csv") %>%
    # recode categorical vars
    mutate(day_of_week = factor(day_of_week,
                                levels = c("Mon", "Tue", "Wed","Thu",
                                           "Fri","Sat","Sun")),
           month = factor(month, levels=c("Sep", "Oct","Nov")))

#capmetro %>% colnames()
```


## Saratoga House Prices

```{r saratoga_import, cache=TRUE}
saratoga_houses <- mosaicData::SaratogaHouses

# features
saratoga_houses %>% colnames()

# head
saratoga_houses %>% head()

# Histogtam of saratoga house prices
saratoga_houses %>%
  ggplot(aes(x = price)) +
  geom_histogram(bins = 50, fill = "dodgerblue", color = "black") + 
  labs(x="Price", y="Count",
     title="Distribution of Price",
     subtitle="Saratoga Houses",
     caption="Source: MosaicData") +
  theme_ipsum(grid="Y")
```

### Build Model

We start with a very simple model. The first step is to create the train/test split.

```{r saratoga_test_train, cache=TRUE}
set.seed(395)
saratoga_split <- initial_split(saratoga_houses, strata = "price", prop = 0.75)

saratoga_train <- training(saratoga_split)
saratoga_test  <- testing(saratoga_split)

dim(saratoga_train)
dim(saratoga_split)
```

Let's use cross-validation to split the training set into k-folds.

```{r, cache=TRUE}
# 3 fold cross validation (for speed)
saratoga_vfold <- vfold_cv(saratoga_train, v = 3, repeats = 1, strata = "price")
saratoga_vfold 
```

We are interested in two different models, a linear regression model and a knn regression model. We start by creating the model specifications.

```{r mod_spec, cache=TRUE}
lm_spec <-
    linear_reg() %>%
    set_mode("regression") %>%
    set_engine("lm")

lm_spec

knn_spec <-
  nearest_neighbor(
    mode = "regression",
    neighbors = tune("K"),
  ) %>%
  set_engine("kknn")

knn_spec
```

Next we put together a tidymodels `workflow()`:

```{r, cache=TRUE}
saratoga_wf <-
  workflow() %>%
  add_formula(price ~ .)

saratoga_wf
```

There is no model yet. Now we can add a model, and fit to the resamples.

```{r, cache=TRUE}
set.seed(350)
lm_rs <- 
  saratoga_wf %>%
  add_model(lm_spec) %>%
  fit_resamples(
    resamples = saratoga_vfold,
    control = control_resamples(save_pred = TRUE)
  )

lm_rs
```

Second, we can fit the knn model. This one requires a bit more work.

```{r, cache=TRUE}
set.seed(350)
# feature engineering
knn_rec <- 
  recipe(price ~ ., data = saratoga_train)

# workflow
knn_wf <- 
  workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_rec)

gridvals <- tibble(K = seq(1, 200))

knn_rs <- 
  knn_wf %>%
  tune_grid(
    resamples = saratoga_vfold,
    grid = gridvals,
    control = control_resamples(save_pred = TRUE)
  ) 

knn_rs
```
Now we have fit each of the candidate models to the resampled training set. For the knn regression, we take the *minimum* RMSE to find the best setting for the number of neighbors.

```{r, cache=TRUE}
set.seed(350)
# show only the row of minimum RMSE
knn_min <- knn_rs %>%
  collect_metrics() %>% 
  filter(.metric == "rmse") %>%
  filter(mean == min(mean))
knn_min
```

### Evaluate Model

First, we evaluate the linear regression model.

```{r, cache=TRUE}
final_lm_wf <- 
  saratoga_wf %>%
  add_model(lm_spec) 
  
lm_fit <- 
  final_lm_wf %>%
  last_fit(split = saratoga_split)

lm_fit %>% collect_metrics()

lm_results <-
  lm_fit %>%
  collect_predictions()

# view results
lm_results
```

For the linear model, we can look at coefficient estimates.

```{r, cache=TRUE}
lm_fit$.workflow[[1]] %>% 
  tidy() %>% 
  kable(digits = 4, "pipe") 
```

And we can plot the results.

```{r, cache=TRUE}
lm_results %>%
  #unnest(.predictions) %>%
  ggplot(aes(.pred, price)) +
  geom_abline(lty = 2, color = "tomato", size = 1) +
  geom_point(alpha = 0.5, color = "dodgerblue") +
  labs(
    title = 'Linear Regression Results - Saratoga Test Set',
    x = "Truth",
    y = "Predicted price",
    color = NULL
  ) + 
  theme_ipsum()
```

Next, we evaluate the KNN model.

```{r, cache=TRUE}
final_knn_wf <- 
  knn_wf %>% 
  finalize_workflow(knn_min)

knn_fit <- 
  final_knn_wf %>% 
  last_fit(split = saratoga_split)

knn_fit %>% collect_metrics()

# predictions
knn_results <- 
  knn_fit %>% 
  collect_predictions()

# view results
knn_results
```

And we can plot the results.

```{r, cache=TRUE}
knn_results %>%
  #unnest(.predictions) %>%
  ggplot(aes(.pred, price)) +
  geom_abline(lty = 2, color = "tomato", size = 1) +
  geom_point(alpha = 0.5, color = "dodgerblue") +
  labs(
    title = 'KNN Regression Results - Saratoga Test Set',
    x = "Truth",
    y = "Predicted price",
    color = NULL
  ) + 
  theme_ipsum()
```

## Classification and retrospective sampling

```{r}
german_credit <- 
  read_data("german_credit.csv") %>% 
  select(-X1)
```

### Build logistic regression model

```{r}
german_credit %>%
  group_by(Default, history) %>%
  add_tally() %>% 
  rename(num_default = n) %>% 
  distinct(history, num_default) %>%
  ungroup() %>%
  group_by(history) %>%
  mutate(tot_default = sum(num_default),
         prob_default = (num_default / tot_default) * 100) %>%
  filter(Default == 1) %>%
  ggplot() +
  geom_col(aes(x = history, y = prob_default, 
             fill = history)) + 
  scale_y_continuous(labels = function(x) paste0(x, "%")) +
  scale_x_discrete(labels = c("Good", "Poor", "Terrible")) +
  labs(x = "History", y = "Probability of Default") +
  theme_ipsum(grid="Y") + 
  theme(legend.title = element_blank(),
        legend.position = "None")
```


Use the following:

- `duration + amount + installment + age + history + purpose + foreign`

```{r}

```




## Children and Hotel Reservations


## Session Information 

```{r sysinfo}
sessionInfo()
```

