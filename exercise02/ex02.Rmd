---
title: "Exercise 02"
author: "Scott Cohn"
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(scipen = 999) 

library(tidyverse)
library(tidymodels)
library(tidyquant)
library(skimr)
library(kknn)
library(glmnet)
library(lubridate)
library(scales)
library(patchwork)
library(hrbrthemes)
library(kableExtra)
library(gcookbook)
library(mosaic)
library(mosaicData)
```


```{r}
# funcs

read_data <- function(df) {
  #' read data from git url
  #' INPUT: data set name
  #' OUTPUT: dataframe
  full_path <- paste("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/", 
                     df, sep = "")
  df <- read_csv(full_path)
  return(df)
}

```

## Visualization

```{r capmetro_import, eval=FALSE}
# he hasn't uploaded data yet
capmetro <- read_data("capmetro_UT.csv") %>%
    # recode categorical vars
    mutate(day_of_week = factor(day_of_week,
                                levels = c("Mon", "Tue", "Wed","Thu",
                                           "Fri","Sat","Sun")),
           month = factor(month, levels=c("Sep", "Oct","Nov")))

#capmetro %>% colnames()
```


## Saratoga House Prices

```{r saratoga_import, cache=TRUE}
saratoga_houses <- mosaicData::SaratogaHouses

skimr::skim(saratoga_houses)

# Histogtam of saratoga house prices
saratoga_houses %>%
  ggplot(aes(x = price)) +
  geom_histogram(bins = 50, fill = "dodgerblue", color = "black") + 
  labs(x="Price", y="Count",
     title="Distribution of Price",
     subtitle="Saratoga Houses",
     caption="Source: MosaicData") +
  theme_ipsum(grid="Y")
```

### Build Model

We start with a very simple model. The first step is to create the train/test split.

```{r saratoga_test_train, cache=TRUE}
set.seed(395)
saratoga_split <- initial_split(saratoga_houses, strata = "price", prop = 0.75)

saratoga_train <- training(saratoga_split)
saratoga_test  <- testing(saratoga_split)

dim(saratoga_train)
dim(saratoga_split)
```

Let's use cross-validation to split the training set into k-folds.

```{r, cache=TRUE}
# 3 fold cross validation (for speed)
saratoga_vfold <- vfold_cv(saratoga_train, v = 3, repeats = 1, strata = "price")
saratoga_vfold 
```

We are interested in two different models, a linear regression model and a knn regression model. We start by creating the model specifications.

```{r mod_spec, cache=TRUE}
lm_spec <-
    linear_reg() %>%
    set_mode("regression") %>%
    set_engine("lm")

lm_spec

knn_spec <-
  nearest_neighbor(
    mode = "regression",
    neighbors = tune("K"),
  ) %>%
  set_engine("kknn")

knn_spec
```

Next we put together a tidymodels `workflow()`:

```{r, cache=TRUE}
# feature engineering
saratoga_wf <-
  workflow() %>%
  add_formula(price ~ .)

saratoga_wf
```

There is no model yet. Now we can add a model, and fit to the resamples.

```{r, cache=TRUE}
set.seed(350)
lm_rs <- 
  saratoga_wf %>%
  add_model(lm_spec) %>%
  fit_resamples(
    resamples = saratoga_vfold,
    control = control_resamples(save_pred = TRUE)
  )

lm_rs
```

Second, we can fit the KNN model. This one requires a bit more work.

```{r, cache=TRUE}
set.seed(350)
# feature engineering
knn_rec <- 
  recipe(price ~ ., data = saratoga_train)

# workflow
knn_wf <- 
  workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_rec)

# hyperparametering tuning
gridvals <- tibble(K = seq(1, 200))

knn_rs <- 
  knn_wf %>%
  tune_grid(
    resamples = saratoga_vfold,
    grid = gridvals,
    control = control_resamples(save_pred = TRUE)
  ) 

knn_rs
```

Now we have fit each of the candidate models to the resampled training set. For the KNN regression, we take the *minimum* RMSE to find the best setting for the number of neighbors.

```{r, cache=TRUE}
set.seed(350)
# hyperparameter tuning
# show only the row of minimum RMSE
knn_min <- knn_rs %>%
  collect_metrics() %>% 
  filter(.metric == "rmse") %>%
  filter(mean == min(mean))
knn_min
```

### Evaluate Model

First, we evaluate the linear regression model.

```{r, cache=TRUE}
final_lm_wf <- 
  saratoga_wf %>%
  add_model(lm_spec) 
  
lm_fit <- 
  final_lm_wf %>%
  last_fit(split = saratoga_split)

lm_fit %>% collect_metrics()

lm_results <-
  lm_fit %>%
  collect_predictions()

# view results
lm_results
```

For the linear model, we can look at coefficient estimates.

```{r, cache=TRUE}
lm_fit$.workflow[[1]] %>% 
  tidy() %>% 
  kable(digits = 4, "pipe") 
```

And we can plot the results.

```{r, cache=TRUE}
lm_results %>%
  #unnest(.predictions) %>%
  ggplot(aes(.pred, price)) +
  geom_abline(lty = 2, color = "tomato", size = 1) +
  geom_point(alpha = 0.5, color = "dodgerblue") +
  labs(
    title = 'Linear Regression Results - Saratoga Test Set',
    x = "Truth",
    y = "Predicted price",
    color = NULL
  ) + 
  theme_ipsum()
```

Next, we evaluate the KNN model.

```{r, cache=TRUE}
final_knn_wf <- 
  knn_wf %>% 
  finalize_workflow(knn_min)

knn_fit <- 
  final_knn_wf %>% 
  last_fit(split = saratoga_split)

knn_fit %>% collect_metrics()

# predictions
knn_results <- 
  knn_fit %>% 
  collect_predictions()

# view results
knn_results
```

And we can plot the results.

```{r, cache=TRUE}
knn_results %>%
  #unnest(.predictions) %>%
  ggplot(aes(.pred, price)) +
  geom_abline(lty = 2, color = "tomato", size = 1) +
  geom_point(alpha = 0.5, color = "dodgerblue") +
  labs(
    title = 'KNN Regression Results - Saratoga Test Set',
    x = "Truth",
    y = "Predicted price",
    color = NULL
  ) + 
  theme_ipsum()
```

## Classification and retrospective sampling

```{r}
german_credit <- 
  read_data("german_credit.csv") %>% 
  select(-1) %>%
  # set outcome as factor 
  mutate(Default = as.factor(Default))

skimr::skim(german_credit)
```

### Build logistic regression model

First, we create a bar-plot of the percentage of default by credit history.

```{r}
german_credit %>%
  group_by(Default, history) %>%
  add_tally() %>% 
  rename(num_default = n) %>% 
  distinct(history, num_default) %>%
  ungroup() %>%
  group_by(history) %>%
  mutate(tot_default = sum(num_default),
         prob_default = (num_default / tot_default) * 100) %>%
  filter(Default == 1) %>%
  ggplot() +
  geom_col(aes(x = history, y = prob_default, 
             fill = history)) + 
  scale_y_continuous(labels = function(x) paste0(x, "%")) +
  scale_x_discrete(labels = c("Good", "Poor", "Terrible")) +
  labs(x = "History", y = "Probability of Default") +
  theme_ipsum(grid="Y") + 
  theme(legend.title = element_blank(),
        legend.position = "None")
```

Next, we create the train/test splits.

```{r}
set.seed(395)
german_split <- initial_split(german_credit, strata = "Default", prop = 0.75)

german_train <- training(german_split)
german_test  <- testing(german_split)

# 3 fold cross validation (for speed)
german_vfold <- vfold_cv(german_train, v = 3, repeats = 1, strata = "Default")
german_vfold 
```

From the problem statement, we use the following features:

- `duration + amount + installment + age + history + purpose + foreign`

So the first step is to construct the model engine.

```{r}
log_spec <-
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification") 

log_spec
```

Next, we define a recipe and workflow.

```{r}
set.seed(350)

# varlist to keep
varlist <- c("Default", "duration", "amount", "installment", "age", 
             "history", "purpose", "foreign")

# recipe // feature engineering
log_rec <- 
  recipe(Default ~ ., data = german_train) %>%
  # remove vars not in varlist
  step_rm(setdiff(colnames(german_credit), varlist)) %>%
  #step_num2factor(default) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
  
# workflow
log_wf <- 
  workflow() %>%
  add_model(log_spec) %>%
  add_recipe(log_rec)
```

There is no model yet. Now we can add a model, and fit to the resamples. First, we need to create the grid for tuning.

```{r}
log_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

# lowest penalty values
log_grid %>% top_n(-5) 

# highest penalty values
log_grid %>% top_n(5)  
```


```{r, cache=TRUE}
set.seed(350)

log_rs <- 
  log_wf %>% 
  tune_grid(german_vfold,
            grid = log_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

log_rs
```

We can look at the area under the ROC curve against the range of penalty values.

```{r}
log_rs %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number()) + 
  theme_ipsum()
```
Here we see that model performance is typically better at smaller values. This indiates that the majority of predictors are important in this model. 

```{r}
top_models <-
  log_rs %>% 
  show_best("roc_auc", n = 15) %>% 
  arrange(penalty) 

top_models 
```

```{r}
log_rs %>%
  select_best()
```
It seems model 15 is our best choice. Let's look at an ROC plot.

```{r}
log_best <- 
  log_rs %>% 
  collect_metrics() %>% 
  arrange(penalty) %>% 
  slice(15)

log_auc <- 
  log_rs %>% 
  collect_predictions(parameters = log_best) %>% 
  roc_curve(Default, .pred_0) %>% 
  mutate(model = "Logistic Regression")

autoplot(log_auc)
```

This model is fine, but not that good.


```{r}
final_log_wf <- 
  log_wf %>% 
  finalize_workflow(log_best)

log_fit <- 
  final_log_wf %>% 
  last_fit(split = german_split)

log_fit %>% collect_metrics()

# predictions
log_results <- 
  log_fit %>% 
  collect_predictions()

# view results
log_results

# ROC curve
log_results %>% 
  roc_curve(Default, .pred_0) %>% 
  autoplot()
```



## Children and Hotel Reservations


## Session Information 

```{r sysinfo}
sessionInfo()
```

